{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAlkRM5av7aR8Hihx4NzO7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wywEmBCu3THL","executionInfo":{"status":"ok","timestamp":1756354161704,"user_tz":-420,"elapsed":131,"user":{"displayName":"Patchara Phookheaw","userId":"01883219134215981788"}},"outputId":"65e3eaa3-3ad2-4054-b0db-9d27b7b25238"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Created 06_sentiment_pipeline.ipynb และ news_th_template.csv (ถ้าไม่มีไฟล์ข่าวจริง)\n"]}],"source":["import nbformat as nbf\n","from pathlib import Path\n","\n","# path\n","nb_path = Path(\"06_sentiment_pipeline.ipynb\")\n","\n","nb = nbf.v4.new_notebook()\n","nb[\"metadata\"][\"colab\"] = {\"provenance\": []}\n","\n","nb.cells = [\n","    nbf.v4.new_markdown_cell(\"# 06 — Sentiment Pipeline (Thai, lexicon v1, dummy-ready)\"),\n","    nbf.v4.new_code_cell(\"\"\"\\\n","%pip -q install pandas numpy matplotlib pythainlp\n","\"\"\"),\n","    nbf.v4.new_code_cell(\"\"\"\\\n","import os, pandas as pd, numpy as np, matplotlib.pyplot as plt\n","from pythainlp.tokenize import word_tokenize\n","\n","# โหลด dataset หลัก\n","BASE_FILE = \"dataset_features_labels.csv\"\n","if not os.path.exists(BASE_FILE):\n","    raise FileNotFoundError(\"❌ ไม่พบ dataset_features_labels.csv — โปรดรัน 02_feature_label.ipynb ก่อน\")\n","\n","df = pd.read_csv(BASE_FILE, index_col=0, parse_dates=True)\n","print(\"Loaded base dataset:\", df.shape)\n","\"\"\"),\n","    nbf.v4.new_code_cell(\"\"\"\\\n","# Lexicon\n","POS = {\"บวก\",\"พุ่ง\",\"กำไร\",\"เติบโต\",\"ฟื้นตัว\",\"ดี\",\"ทะลุ\",\"สูงขึ้น\",\"แข็งแกร่ง\",\"สดใส\"}\n","NEG = {\"ลบ\",\"ร่วง\",\"ขาดทุน\",\"ชะลอ\",\"ถดถอย\",\"แย่\",\"ดิ่ง\",\"ต่ำลง\",\"วิกฤต\",\"ซบเซา\"}\n","\n","def sentiment_score_th(text: str) -> float:\n","    toks = word_tokenize(str(text), keep_whitespace=False)\n","    if not toks: return 0.0\n","    s = sum((t in POS) - (t in NEG) for t in toks)\n","    return s / np.sqrt(len(toks))\n","\"\"\"),\n","    nbf.v4.new_code_cell(\"\"\"\\\n","NEWS_FILE = \"news_th.csv\"\n","if os.path.exists(NEWS_FILE):\n","    news = pd.read_csv(NEWS_FILE)\n","    news[\"date\"] = pd.to_datetime(news[\"date\"])\n","else:\n","    # dummy news\n","    news = pd.DataFrame({\n","        \"date\": pd.date_range(df.index.min(), periods=5, freq=\"7D\"),\n","        \"symbol\": [\"^SET50\"]*5,\n","        \"text\": [\n","            \"หุ้นไทยพุ่งแรงหลังเศรษฐกิจฟื้นตัว\",\n","            \"ตลาดปรับตัวลบจากความกังวลเศรษฐกิจถดถอย\",\n","            \"ผลประกอบการกำไรเติบโต\",\n","            \"ความเชื่อมั่นชะลอตัว หุ้นร่วง\",\n","            \"แนวโน้มดีขึ้นอย่างต่อเนื่อง\",\n","        ]\n","    })\n","    news.to_csv(\"news_th_template.csv\", index=False)\n","    print(\"⚠️ ไม่มี news_th.csv → สร้าง news_th_template.csv ให้แล้ว\")\n","\"\"\"),\n","    nbf.v4.new_code_cell(\"\"\"\\\n","# Sentiment\n","news[\"score\"] = news[\"text\"].apply(sentiment_score_th)\n","sent_daily = news.groupby(pd.Grouper(key=\"date\", freq=\"D\"))[\"score\"].mean().rename(\"Sentiment_Daily\").to_frame()\n","sent_daily.to_csv(\"sent_daily_preview.csv\")\n","df_out = df.join(sent_daily, how=\"left\").fillna(0.0)\n","df_out.to_csv(\"dataset_features_labels_with_sentiment.csv\")\n","print(\"✅ Saved dataset_features_labels_with_sentiment.csv\", df_out.shape)\n","\"\"\"),\n","]\n","\n","# save notebook\n","nb_path.write_text(nbf.writes(nb), encoding=\"utf-8\")\n","\n","print(\"✅ Created 06_sentiment_pipeline.ipynb และ news_th_template.csv (ถ้าไม่มีไฟล์ข่าวจริง)\")\n"]},{"cell_type":"code","source":["import nbformat as nbf\n","from pathlib import Path\n","\n","nb_path = Path(\"05_transformer_upgrade.ipynb\")\n","\n","nb = nbf.v4.new_notebook()\n","nb[\"metadata\"][\"colab\"] = {\"provenance\": []}\n","cells = []\n","\n","# Title\n","cells.append(nbf.v4.new_markdown_cell(\"# 05 — Transformer Upgrade (LSTM vs Transformer, with Sentiment & Indicators)\\n\"\n","\"Workflow: เตรียมข้อมูล → LSTM benchmark → Transformer upgrade (multi-head, multi-block) → เปรียบเทียบผล + Backtest\"))\n","\n","# Setup\n","cells.append(nbf.v4.new_code_cell(\"\"\"\\\n","%pip -q install pandas numpy matplotlib scikit-learn tensorflow==2.*\n","\"\"\"))\n","\n","# Imports + Data Load\n","cells.append(nbf.v4.new_code_cell(\"\"\"\\\n","import numpy as np, pandas as pd, matplotlib.pyplot as plt, tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import os\n","\n","# Load dataset (with sentiment if available)\n","FILE = \"dataset_features_labels_with_sentiment.csv\"\n","if not os.path.exists(FILE):\n","    FILE = \"dataset_features_labels.csv\"\n","\n","df = pd.read_csv(FILE, index_col=0, parse_dates=True)\n","print(\"Loaded dataset:\", df.shape)\n","df.head(3)\n","\"\"\"))\n","\n","# Train/Val/Test split\n","cells.append(nbf.v4.new_code_cell(\"\"\"\\\n","FEATURES = [c for c in df.columns if c != \"Target\"]\n","X_all = df[FEATURES].values\n","y_all = df[\"Target\"].values\n","\n","n = len(df); n_train = int(n*0.7); n_val = int(n*0.85)\n","X_tr, y_tr = X_all[:n_train], y_all[:n_train]\n","X_va, y_va = X_all[n_train:n_val], y_all[n_train:n_val]\n","X_te, y_te = X_all[n_val:], y_all[n_val:]\n","\n","# scaling\n","scaler = StandardScaler().fit(X_tr)\n","X_tr = scaler.transform(X_tr); X_va = scaler.transform(X_va); X_te = scaler.transform(X_te)\n","\n","# make sequences\n","def make_seq(X, y, win=20):\n","    xs, ys = [], []\n","    for i in range(win, len(X)):\n","        xs.append(X[i-win:i])\n","        ys.append(y[i])\n","    return np.array(xs), np.array(ys)\n","\n","WIN = 20\n","Xtr, ytr = make_seq(X_tr, y_tr, WIN)\n","Xva, yva = make_seq(X_va, y_va, WIN)\n","Xte, yte = make_seq(X_te, y_te, WIN)\n","idx_test = df.index[n_val+WIN:]\n","\n","Xtr.shape, Xva.shape, Xte.shape, len(idx_test)\n","\"\"\"))\n","\n","# LSTM Benchmark\n","cells.append(nbf.v4.new_code_cell(\"\"\"\\\n","from tensorflow.keras import layers, models, callbacks\n","\n","def build_lstm(input_shape, units=64, dropout=0.2):\n","    m = models.Sequential([\n","        layers.Input(shape=input_shape),\n","        layers.LSTM(units),\n","        layers.Dropout(dropout),\n","        layers.Dense(32, activation=\"relu\"),\n","        layers.Dense(1)\n","    ])\n","    m.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","    return m\n","\n","lstm = build_lstm(Xtr.shape[1:])\n","es = callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\")\n","lstm.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=80, batch_size=64, callbacks=[es], verbose=0)\n","yhat_lstm = lstm.predict(Xte).ravel()\n","\"\"\"))\n","\n","# Transformer Upgrade\n","cells.append(nbf.v4.new_code_cell(\"\"\"\\\n","from tensorflow.keras import layers, models, optimizers\n","\n","def sinusoidal_position_encoding(seq_len, d_model):\n","    pos = np.arange(seq_len)[:, None]\n","    i = np.arange(d_model)[None, :]\n","    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n","    angles = pos * angle_rates\n","    pe = np.zeros((seq_len, d_model))\n","    pe[:, 0::2] = np.sin(angles[:, 0::2])\n","    pe[:, 1::2] = np.cos(angles[:, 1::2])\n","    return tf.constant(pe, dtype=tf.float32)\n","\n","def build_transformer(input_shape, num_layers=2, num_heads=4, d_model=64, ff_dim=128, dropout=0.2):\n","    seq_len, n_feat = input_shape\n","    inp = layers.Input(shape=input_shape)\n","    x = layers.Dense(d_model)(inp)\n","    pe = sinusoidal_position_encoding(seq_len, d_model)\n","    x = x + pe\n","    for _ in range(num_layers):\n","        attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(x, x)\n","        attn = layers.Dropout(dropout)(attn)\n","        x = layers.LayerNormalization(epsilon=1e-6)(x + attn)\n","        f = layers.Dense(ff_dim, activation=\"relu\")(x)\n","        f = layers.Dropout(dropout)(f)\n","        f = layers.Dense(d_model)(f)\n","        x = layers.LayerNormalization(epsilon=1e-6)(x + f)\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dense(64, activation=\"relu\")(x)\n","    out = layers.Dense(1)(x)\n","    model = models.Model(inp, out)\n","    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","    return model\n","\n","trf = build_transformer(Xtr.shape[1:], num_layers=3, num_heads=4, d_model=64, ff_dim=128, dropout=0.2)\n","es2 = callbacks.EarlyStopping(patience=12, restore_best_weights=True, monitor=\"val_loss\")\n","trf.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=100, batch_size=64, callbacks=[es2], verbose=0)\n","yhat_tf = trf.predict(Xte).ravel()\n","\"\"\"))\n","\n","# Metrics + Backtest\n","cells.append(nbf.v4.new_code_cell(\"\"\"\\\n","import numpy as np, pandas as pd, matplotlib.pyplot as plt\n","\n","def metrics(y_true, y_pred):\n","    return {\n","        \"RMSE\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n","        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n","        \"R2\": float(r2_score(y_true, y_pred)),\n","        \"DA\": float((np.sign(y_true)==np.sign(y_pred)).mean())\n","    }\n","\n","m_lstm = metrics(yte, yhat_lstm)\n","m_trf  = metrics(yte, yhat_tf)\n","\n","print(\"LSTM:\", m_lstm)\n","print(\"Transformer:\", m_trf)\n","\n","bt = pd.DataFrame(index=idx_test)\n","bt[\"Return\"] = yte\n","bt[\"LSTM\"] = np.where(yhat_lstm>0, 1, -1) * bt[\"Return\"]\n","bt[\"Transformer\"] = np.where(yhat_tf>0, 1, -1) * bt[\"Return\"]\n","\n","bt[[\"Return\",\"LSTM\",\"Transformer\"]].cumsum().plot(figsize=(10,5), title=\"Backtest Comparison\")\n","plt.show()\n","\"\"\"))\n","\n","nb[\"cells\"] = cells\n","nb_path.write_text(nbf.writes(nb), encoding=\"utf-8\")\n","\n","print(\"✅ Created\", nb_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qtpn0IP3njj","executionInfo":{"status":"ok","timestamp":1756354218486,"user_tz":-420,"elapsed":13,"user":{"displayName":"Patchara Phookheaw","userId":"01883219134215981788"}},"outputId":"b016c0ea-1b76-4db3-d5be-4887b3a35b57"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Created 05_transformer_upgrade.ipynb\n"]}]}]}